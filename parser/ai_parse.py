import argparse
import json
from datetime import datetime
from typing import List, Optional, Dict
import re
from natasha import (
    Segmenter,
    MorphVocab,
    NewsEmbedding,
    NewsMorphTagger,
    NewsSyntaxParser,
    NewsNERTagger,
    Doc,
    DatesExtractor,
    MoneyExtractor,
    AddrExtractor,
)

# Initialize Natasha's components
segmenter = Segmenter()
morph_vocab = MorphVocab()
emb = NewsEmbedding()
morph_tagger = NewsMorphTagger(emb)
syntax_parser = NewsSyntaxParser(emb)
ner_tagger = NewsNERTagger(emb)
dates_extractor = DatesExtractor(morph_vocab)
money_extractor = MoneyExtractor(morph_vocab)
addr_extractor = AddrExtractor(morph_vocab)


class Event:
    def __init__(self):
        self.title: str = ""
        self.description: Optional[str] = None
        self.age_min: Optional[int] = None
        self.age_max: Optional[int] = None
        self.discipline: List[str] = []
        self.start_date: Optional[str] = None
        self.end_date: Optional[str] = None
        self.location: List[Dict[str, Optional[str]]] = []
        self.participant_count: Optional[int] = None

    def to_dict(self) -> dict:
        return {
            "title": self.title,
            "description": self.description,
            "age_min": self.age_min,
            "age_max": self.age_max,
            "discipline": self.discipline,
            "start_date": self.start_date,
            "end_date": self.end_date,
            "location": self.location,
            "participant_count": self.participant_count,
        }


def extract_age_range(text: str) -> tuple[Optional[int], Optional[int]]:
    """Extract age range from text using Natasha NLP."""
    doc = Doc(text)
    doc.segment(segmenter)
    doc.tag_morph(morph_tagger)

    text_lower = text.lower()

    # First check for team size mentions (these are not age ranges)
    team_size_patterns = [
        r"–∫–æ–º–∞–Ω–¥[–∞-—è]*\s+(?:–∏–∑|–ø–æ|—á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å—é)\s+\d+-\d+",
        r"—Å–æ—Å—Ç–∞–≤[–∞-—è]*\s+(?:–∏–∑|–ø–æ)\s+\d+-\d+",
    ]

    for pattern in team_size_patterns:
        if re.search(pattern, text_lower):
            continue  # Skip these matches as they're about team size, not age

    # Common patterns for age ranges
    age_patterns = [
        r"(?:–æ—Ç\s+)?(\d+)(?:\s*-\s*|\s+–¥–æ\s+)(\d+)(?:\s+–ª–µ—Ç)?",
        r"(?:–≤\s+–≤–æ–∑—Ä–∞—Å—Ç–µ\s+)?(\d+)\s*-\s*(\d+)\s+–ª–µ—Ç",
        r"–≤–æ–∑—Ä–∞—Å—Ç[–∞-—è]*\s+(?:–æ—Ç\s+)?(\d+)(?:\s*-\s*|\s+–¥–æ\s+)(\d+)",
        r"–¥–ª—è\s+(?:—É—á–∞—Å—Ç–Ω–∏–∫–æ–≤|–¥–µ—Ç–µ–π|—à–∫–æ–ª—å–Ω–∏–∫–æ–≤|—Ä–µ–±—è—Ç)\s+(?:–æ—Ç\s+)?(\d+)(?:\s*-\s*|\s+–¥–æ\s+)(\d+)",
        r"(\d+)\+",
        r"—Å—Ç–∞—Ä—à–µ\s+(\d+)",
        r"–æ—Ç\s+(\d+)\s+–ª–µ—Ç",
        r"–¥–æ—Å—Ç–∏–≥—à–∏—Ö\s+(\d+)",
        r"–≤–æ–∑—Ä–∞—Å—Ç[–∞-—è]*\s+(\d+)\+",
    ]

    # First try to find explicit age ranges
    for pattern in age_patterns:
        match = re.search(pattern, text_lower)
        if match:
            if len(match.groups()) == 2:
                age1, age2 = int(match.group(1)), int(match.group(2))
                # Validate the age range
                if 5 <= age1 <= 100 and 5 <= age2 <= 100:  # Reasonable age limits
                    return min(age1, age2), max(age1, age2)
            else:
                age = int(match.group(1))
                if 5 <= age <= 100:  # Reasonable age limit
                    if "+" in pattern or "—Å—Ç–∞—Ä—à–µ" in pattern or "–æ—Ç" in pattern:
                        return age, None
                    return age, age

    # Use Natasha's morphological analysis to find age-related words
    age_related_words = {
        "—à–∫–æ–ª—å–Ω–∏–∫": (14, 18),
        "—Å—Ç—É–¥–µ–Ω—Ç": (17, 25),
        "–º–æ–ª–æ–¥–µ–∂—å": (14, 35),
        "—é–Ω–∏–æ—Ä": (14, 18),
        "–≤–∑—Ä–æ—Å–ª—ã–π": (18, None),
    }

    # Look for age-related words in the text
    for token in doc.tokens:
        if token.pos == "NOUN" and token.lemma in age_related_words:
            return age_related_words[token.lemma]

    # Context-based inference
    if "—Ö–∞–∫–∞—Ç–æ–Ω" in text_lower or "–ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ" in text_lower:
        if "—Ç–∞–ª–∞–Ω—Ç–ª–∏–≤" in text_lower and "—Å–ø–æ—Ä—Ç—Å–º–µ–Ω" in text_lower:
            return 14, None
        return 16, None
    elif "–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ" in text_lower:
        if "—à–∫–æ–ª—å–Ω–∏–∫" in text_lower:
            return 14, 18
        elif "—Å—Ç—É–¥–µ–Ω—Ç" in text_lower:
            return 17, 25
        return 14, None

    return None, None


def extract_date(text: str) -> Optional[str]:
    """Extract date from text using Natasha's DatesExtractor."""
    doc = Doc(text)
    doc.segment(segmenter)

    # Convert generator to list
    matches = list(dates_extractor(text))

    if matches:
        # Get the first date mention
        match = matches[0]
        date_value = match.fact

        # Convert to datetime and ISO format
        try:
            # Check if we have a complete date
            if (
                getattr(date_value, "year", None) is not None
                and getattr(date_value, "month", None) is not None
                and getattr(date_value, "day", None) is not None
            ):
                return datetime(
                    date_value.year, date_value.month, date_value.day
                ).isoformat()
        except (ValueError, AttributeError):
            pass

    # Fallback to regex patterns if Natasha doesn't find dates
    patterns = [
        r"(\d{4})-(\d{2})-(\d{2})",
        r"(\d{2})\.(\d{2})\.(\d{4})",
        r"(\d{1,2})\s+(—è–Ω–≤(?:–∞—Ä—è)?|—Ñ–µ–≤(?:—Ä–∞–ª—è)?|–º–∞—Ä(?:—Ç–∞)?|–∞–ø—Ä(?:–µ–ª—è)?|–º–∞—è|–∏—é–Ω(?:—è)?|–∏—é–ª(?:—è)?|–∞–≤–≥(?:—É—Å—Ç–∞)?|—Å–µ–Ω(?:—Ç—è–±—Ä—è)?|–æ–∫—Ç(?:—è–±—Ä—è)?|–Ω–æ—è(?:–±—Ä—è)?|–¥–µ–∫(?:–∞–±—Ä—è)?)\s+(\d{4})",
    ]

    months = {
        "—è–Ω–≤": 1,
        "—Ñ–µ–≤": 2,
        "–º–∞—Ä": 3,
        "–∞–ø—Ä": 4,
        "–º–∞–π": 5,
        "–∏—é–Ω": 6,
        "–∏—é–ª": 7,
        "–∞–≤–≥": 8,
        "—Å–µ–Ω": 9,
        "–æ–∫—Ç": 10,
        "–Ω–æ—è": 11,
        "–¥–µ–∫": 12,
    }

    text_lower = text.lower()
    for pattern in patterns:
        match = re.search(pattern, text_lower)
        if match:
            try:
                if len(match.groups()) == 3:
                    if match.group(2) in months or match.group(2)[:3] in months:
                        day = int(match.group(1))
                        month = months.get(match.group(2)[:3], 1)
                        year = int(match.group(3))
                    else:
                        year = int(match.group(1))
                        month = int(match.group(2))
                        day = int(match.group(3))
                    return datetime(year, month, day).isoformat()
            except (ValueError, IndexError):
                continue

    return None


def normalize_location_name(name: str) -> str:
    """Normalize location name by removing case endings and standardizing format."""
    # Remove common case endings
    endings = [
        "–æ–≥–æ",
        "–µ–≥–æ",
        "–æ–º—É",
        "–µ–º—É",
        "–æ–π",
        "–µ–π",
        "–æ–º",
        "–µ–º",
        "–∞—è",
        "—è—è",
        "—É—é",
        "—é—é",
        "—ã–µ",
        "–∏–µ",
        "—ã–π",
        "–∏–π",
    ]
    name_lower = name.lower()

    # Skip normalization for known city names
    city_mapping = {
        "–º—Å–∫": "–ú–æ—Å–∫–≤–∞",
        "—Å–ø–±": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
        "–ø–∏—Ç–µ—Ä": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
        "–∫–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫": "–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥",
        "–º–æ—Å–∫–æ–≤—Å–∫": "–ú–æ—Å–∫–≤–∞",
        "—Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å—Å–∫": "–°–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å",
    }

    for key, value in city_mapping.items():
        if name_lower.startswith(key):
            return value

    # Remove case endings
    for ending in endings:
        if name_lower.endswith(ending):
            name = name[: -len(ending)]
            break

    # Capitalize first letter of each word
    words = name.lower().split()
    name = " ".join(word.capitalize() for word in words)

    return name.strip()


def is_valid_location(text: str, location_type: str) -> bool:
    """Check if the text is a valid location name."""
    # Words that should not be considered locations
    invalid_words = {
        "—Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ",
        "—Ç–µ—Å—Ç–µ—Ä—ã",
        "—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–µ",
        "—Ñ–µ–¥–µ—Ä–∞—Ü–∏—è",
        "–≤—Ä–µ–º—è",
        "–≤—Ä–µ–º–µ–Ω–∏",
        "–æ–±–ª–∞—Å—Ç—å",
        "–∫—Ä–∞–π",
        "—Ä–µ—Å–ø—É–±–ª–∏–∫–∞",
        "–æ–∫—Ä—É–≥",
        "—Ä–µ–≥–∏–æ–Ω",
        "—Ç–∏–ø",
        "—Ñ–æ—Ä–º–∞—Ç",
        "—Ä–æ—Å—Å–∏–∏",
        "—Ä—Ñ",
        "—Ä–æ—Å—Å–∏–π—Å–∫–æ–π",
        "—Ä–æ—Å—Å–∏–π—Å–∫–∞—è",
        "—Ä–æ—Å—Å–∏—è",  # Country names should be handled separately
    }

    text_lower = text.lower()

    # Check if it's in the invalid words list
    if text_lower in invalid_words:
        return False

    # Check if it's a time zone reference
    if "–≤—Ä–µ–º—è" in text_lower or "–≤—Ä–µ–º–µ–Ω–∏" in text_lower:
        return False

    # For cities, check minimum length and avoid common false positives
    if location_type == "city":
        if len(text) < 3:  # Too short to be a city name
            return False
        if text_lower.endswith("—Å–∫–æ–µ") or text_lower.endswith("ÔøΩÔøΩ–π"):
            return False
        if text_lower.endswith("—Å–∫–∏–π") or text_lower.endswith("—Å–∫–∞—è"):
            return False
        if text_lower.startswith("–ø–æ ") or text_lower.startswith("–≤ "):
            return False

    return True


def extract_location(text: str) -> List[Dict[str, Optional[str]]]:
    """Extract location information using Natasha NER."""
    doc = Doc(text)
    doc.segment(segmenter)
    doc.tag_ner(ner_tagger)

    locations = []
    text_lower = text.lower()

    # Check for online/offline format first
    if "–æ–Ω–ª–∞–π–Ω" in text_lower:
        return [{"country": "–†–æ—Å—Å–∏—è", "region": None, "city": "–û–Ω–ª–∞–π–Ω"}]

    # Common location patterns
    location_patterns = {
        "city": [
            r"(?:–≥\.|–≥–æ—Ä–æ–¥)\s*([–ê-–Ø][–∞-—è]+(?:-[–ê-–Ø][–∞-—è]+)*)",
            r"–≤\s+(?:–≥–æ—Ä–æ–¥–µ\s+)?([–ê-–Ø][–∞-—è]+(?:-[–ê-–Ø][–∞-—è]+)*)",
            r"–ø—Ä–æ–π–¥[–µ—ë]—Ç\s+–≤\s+(?:–≥–æ—Ä–æ–¥–µ\s+)?([–ê-–Ø][–∞-—è]+(?:-[–ê-–Ø][–∞-—è]+)*)",
            r"—Å–æ—Å—Ç–æ–∏—Ç—Å—è\s+–≤\s+(?:–≥–æ—Ä–æ–¥–µ\s+)?([–ê-–Ø][–∞-—è]+(?:-[–ê-–Ø][–∞-—è]+)*)",
            r"–∞–¥—Ä–µ—Å[—É]?\s*:\s*(?:–≥\.)?\s*([–ê-–Ø][–∞-—è]+(?:-[–ê-–Ø][–∞-—è]+)*)",
            r"–ø–æ\s+–∞–¥—Ä–µ—Å—É\s*(?:–≥\.)?\s*([–ê-–Ø][–∞-—è]+(?:-[–ê-–Ø][–∞-—è]+)*)",
        ],
        "region": [
            r"(?:–æ–±–ª(?:–∞—Å—Ç—å)?|–∫—Ä–∞–π|—Ä–µ—Å–ø—É–±–ª–∏–∫–∞|–æ–∫—Ä—É–≥)\s+([–ê-–Ø][–∞-—è]+(?:-[–ê-–Ø][–∞-—è]+)*)",
            r"([–ê-–Ø][–∞-—è]+(?:-[–ê-–Ø][–∞-—è]+)*)\s+(?:–æ–±–ª(?:–∞—Å—Ç—å)?|–∫—Ä–∞–π|—Ä–µ—Å–ø—É–±–ª–∏–∫–∞|–æ–∫—Ä—É–≥)",
            r"([–ê-–Ø][–∞-—è]+(?:-[–ê-–Ø][–∞-—è]+)*)\s+—Ä–µ–≥–∏–æ–Ω–∞?",
            r"—Ä–µ–≥–∏–æ–Ω–∞?\s+([–ê-–Ø][–∞-—è]+(?:-[–ê-–Ø][–∞-—è]+)*)",
        ],
    }

    # Known cities and their regions
    known_cities = {
        "–º–æ—Å–∫–≤–∞": ("–†–æ—Å—Å–∏—è", "–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–ú–æ—Å–∫–≤–∞"),
        "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥": ("–†–æ—Å—Å–∏—è", "–õ–µ–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥"),
        "–∫–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥": ("–†–æ—Å—Å–∏—è", "–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥"),
        "—Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å": ("–†–æ—Å—Å–∏—è", None, "–°–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å"),
        "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": ("–†–æ—Å—Å–∏—è", "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫"),
        "–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": ("–†–æ—Å—Å–∏—è", "–°–≤–µ—Ä–¥–ª–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥"),
        "–∫–∞–∑–∞–Ω—å": ("–†–æ—Å—Å–∏—è", "–†–µ—Å–ø—É–±–ª–∏–∫–∞ –¢–∞—Ç–∞—Ä—Å—Ç–∞–Ω", "–ö–∞–∑–∞–Ω—å"),
        "–Ω–∏–∂–Ω–∏–π –Ω–æ–≤–≥–æ—Ä–æ–¥": ("–†–æ—Å—Å–∏—è", "–ù–∏–∂–µ–≥–æ—Ä–æ–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥"),
        "—á–µ–ª—è–±–∏–Ω—Å–∫": ("–†–æ—Å—Å–∏—è", "–ß–µ–ª—è–±–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–ß–µ–ª—è–±–∏–Ω—Å–∫"),
        "–æ–º—Å–∫": ("–†–æ—Å—Å–∏—è", "–û–º—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–û–º—Å–∫"),
        "—Å–∞–º–∞—Ä–∞": ("–†–æ—Å—Å–∏—è", "–°–∞–º–∞—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–°–∞–º–∞—Ä–∞"),
        "—Ä–æ—Å—Ç–æ–≤-–Ω–∞-–¥–æ–Ω—É": ("–†–æ—Å—Å–∏—è", "–†–æ—Å—Ç–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É"),
        "—É—Ñ–∞": ("–†–æ—Å—Å–∏—è", "–†–µ—Å–ø—É–±–ª–∏–∫–∞ –ë–∞—à–∫–æ—Ä—Ç–æ—Å—Ç–∞–Ω", "–£—Ñ–∞"),
        "–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫": ("–†–æ—Å—Å–∏—è", "–ö—Ä–∞ÔøΩÔøΩ–Ω–æ—è—Ä—Å–∫–∏–π –∫—Ä–∞–π", "–ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫"),
        "–≤–æ—Ä–æ–Ω–µ–∂": ("–†–æ—Å—Å–∏—è", "–í–æ—Ä–æ–Ω–µ–∂—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–í–æ—Ä–æ–Ω–µ–∂"),
        "–ø–µ—Ä–º—å": ("–†–æ—Å—Å–∏—è", "–ü–µ—Ä–º—Å–∫–∏–π –∫—Ä–∞–π", "–ü–µ—Ä–º—å"),
        "–≤–æ–ª–≥–æ–≥—Ä–∞–¥": ("–†–æ—Å—Å–∏—è", "–í–æ–ª–≥–æ–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "–í–æ–ª–≥–æ–≥—Ä–∞–¥"),
    }

    # First try using addr_extractor
    matches = addr_extractor(text)
    if matches:
        for match in matches:
            addr = match.fact
            location = {"country": None, "region": None, "city": None}

            if hasattr(addr, "country"):
                location["country"] = addr.country
            if hasattr(addr, "region"):
                region = getattr(addr, "region")
                if region and is_valid_location(region, "region"):
                    location["region"] = normalize_location_name(region)
            if hasattr(addr, "city"):
                city = getattr(addr, "city")
                if city and is_valid_location(city, "city"):
                    location["city"] = normalize_location_name(city)

            if any(location.values()):
                locations.append(location)

    # If no locations found with addr_extractor, try patterns and NER
    if not locations:
        current_location = {"country": None, "region": None, "city": None}

        # Try to find cities using patterns
        for pattern in location_patterns["city"]:
            match = re.search(pattern, text)
            if match:
                city = match.group(1).strip()
                if city and is_valid_location(city, "city"):
                    normalized_city = normalize_location_name(city)
                    normalized_city_lower = normalized_city.lower()
                    if normalized_city_lower in known_cities:
                        country, region, city = known_cities[normalized_city_lower]
                        current_location["country"] = country
                        if region:
                            current_location["region"] = region
                        current_location["city"] = city
                    else:
                        current_location["city"] = normalized_city
                    break

        # Try to find regions using patterns
        for pattern in location_patterns["region"]:
            match = re.search(pattern, text)
            if match:
                region = match.group(1).strip()
                if region and is_valid_location(region, "region"):
                    current_location["region"] = normalize_location_name(region)
                    break

        # Use NER for additional location detection
        for span in doc.spans:
            if span.type == "LOC":
                text = span.text
                text_lower = text.lower()

                # Skip already found locations
                if current_location["city"] and text == current_location["city"]:
                    continue
                if current_location["region"] and text == current_location["region"]:
                    continue

                # Check if it's a known city
                normalized_text = normalize_location_name(text)
                normalized_text_lower = normalized_text.lower()

                if normalized_text_lower in known_cities:
                    country, region, city = known_cities[normalized_text_lower]
                    current_location["country"] = country
                    if region:
                        current_location["region"] = region
                    current_location["city"] = city
                    continue

                # Determine location type
                if any(
                    word in text_lower
                    for word in ["–æ–±–ª–∞—Å—Ç—å", "–∫—Ä–∞–π", "—Ä–µ—Å–ø—É–±–ª–∏–∫–∞", "–æ–∫—Ä—É–≥", "—Ä–µ–≥–∏–æ–Ω"]
                ):
                    if not current_location["region"] and is_valid_location(
                        text, "region"
                    ):
                        current_location["region"] = normalize_location_name(text)
                elif "—Ä–æ—Å—Å–∏—è" in text_lower or "—Ä—Ñ" in text_lower:
                    current_location["country"] = "–†–æ—Å—Å–∏—è"
                else:
                    if not current_location["city"] and is_valid_location(text, "city"):
                        current_location["city"] = normalize_location_name(text)

        # Set default country for Russian cities/regions
        if (
            current_location["city"] or current_location["region"]
        ) and not current_location["country"]:
            current_location["country"] = "–†–æ—Å—Å–∏—è"

        # Add location if we found any information
        if any(current_location.values()):
            locations.append(current_location)

    # If still no locations found but we have time zone information
    if not locations:
        tz_patterns = {
            r"(?:–ø–æ|–≤—Ä–µ–º—è)\s+(?:–º—Å–∫|–º–æ—Å–∫–æ–≤—Å–∫–æ–º—É)": (
                "–†–æ—Å—Å–∏—è",
                "–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
                "–ú–æ—Å–∫–≤–∞",
            ),
            r"(?:–ø–æ|–≤—Ä–µ–º—è)\s+–∫–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–æ–º—É": (
                "–†–æ—Å—Å–∏—è",
                "–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
                "–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥",
            ),
        }

        for pattern, (country, region, city) in tz_patterns.items():
            if re.search(pattern, text_lower):
                locations.append({"country": country, "region": region, "city": city})
                break

    return locations


def extract_disciplines(text: str) -> List[str]:
    """Extract programming disciplines using Natasha NLP."""
    doc = Doc(text)
    doc.segment(segmenter)
    doc.tag_morph(morph_tagger)

    disciplines = []
    text_lower = text.lower()

    # First check for explicit discipline mentions in title
    title_match = re.search(r'(?:–≤\s+)?–¥–∏—Å—Ü–∏–ø–ª–∏–Ω[–∞-—è]*\s+[¬´"]([^¬ª"]+)[¬ª"]', text)
    if title_match:
        discipline_name = title_match.group(1).strip().lower()
        if "–ø—Ä–æ–¥—É–∫—Ç–æ–≤" in discipline_name:
            disciplines.append("–ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ")
        elif "–∞–ª–≥–æ—Ä–∏—Ç–º" in discipline_name:
            disciplines.append("–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ")
        elif "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç" in discipline_name:
            disciplines.append("–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")
        elif "—Ä–æ–±–æ—Ç" in discipline_name:
            disciplines.append("–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–æ–±–æ—Ç–æÔøΩÔøΩ–µ—Ö–Ω–∏–∫–∏")
        elif "–±–µ—Å–ø–∏–ª–æ—Ç" in discipline_name or "–±–ø–ª–∞" in discipline_name:
            disciplines.append("–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ—Å–ø–∏–ª–æ—Ç–Ω—ã—Ö –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º")

    # Check for discipline patterns
    discipline_patterns = {
        "–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ": [
            r"–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫[–∞-—è]+\s+–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω[–∞-—è]+",
            r"ICPC",
            r"–æ–ª–∏–º–ø–∏–∞–¥–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ",
            r"—Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ",
            r"ACM",
            r"IOI",
            r"–í–°–û–®.*–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫",
        ],
        "–ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ": [
            r"–ø—Ä–æ–¥—É–∫—Ç–æ–≤[–∞-—è]+\s+–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω[–∞-—è]+",
            r"—Ö–∞–∫–∞—Ç–æ–Ω",
            r"—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–¥—É–∫—Ç[–∞-—è]+",
            r"—Å–æ–∑–¥–∞–Ω–∏[–∞-—è]+\s+–ø—Ä–æ–¥—É–∫—Ç[–∞-—è]+",
            r"—Ö–∞–∫–∞—Ç–æ–Ω.*–ø—Ä–æ–¥—É–∫—Ç",
            r"–ø—Ä–æ–¥—É–∫—Ç.*—Ö–∞–∫–∞—Ç–æ–Ω",
        ],
        "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏": [
            r"–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω[–∞-—è]+\s+–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç[–∞-—è]+",
            r"–∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç[–∞-—è]+",
            r"CTF",
            r"–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç[–∞-—è]+\s+—Å–∏—Å—Ç–µ–º",
        ],
        "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏": [
            r"—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫[–∞-—è]+",
            r"–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω[–∞-—è]+\s+—Ä–æ–±–æ—Ç[–∞-—è]+",
            r"—Ä–æ–±–æ—Ç.*–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω",
        ],
        "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ—Å–ø–∏–ª–æ—Ç–Ω—ã—Ö –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º": [
            r"–±–µ—Å–ø–∏–ª–æ—Ç–Ω[–∞-—è]+\s+(?:–∞–≤–∏–∞—Ü–∏–æ–Ω–Ω[–∞-—è]+\s+)?—Å–∏—Å—Ç–µ–º[–∞-—è]+",
            r"–ë–ü–õ–ê",
            r"–¥—Ä–æ–Ω[–∞-—è]+",
            r"–±–µ—Å–ø–∏–ª–æ—Ç–Ω[–∞-—è]+.*–∞–ø–ø–∞—Ä–∞—Ç",
        ],
    }

    # Check for discipline mentions using patterns
    for discipline, patterns in discipline_patterns.items():
        for pattern in patterns:
            if re.search(pattern, text, re.IGNORECASE):
                disciplines.append(discipline)
                break

    # Special cases based on context
    if "—Ö–∞–∫–∞—Ç–æ–Ω" in text_lower and not disciplines:
        # If it's a hackathon and no discipline is detected, assume it's product programming
        disciplines.append("–ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ")
    elif "ICPC" in text or "ACM" in text:
        # If it's an ICPC/ACM event, it's definitely algorithmic programming
        if "–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ" not in disciplines:
            disciplines.append("–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ")

    # Use morphological analysis to find programming-related terms
    programming_indicators = {
        "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ": set(["–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"]),
        "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞": set(["–ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"]),
        "–∫–æ–¥–∏–Ω–≥": set(["–ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"]),
        "—Ö–∞–∫–∞—Ç–æ–Ω": set(["–ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"]),
        "–∞–ª–≥–æ—Ä–∏—Ç–º": set(["–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"]),
        "–æ–ª–∏–º–ø–∏–∞–¥–∞": set(["–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"]),
        "–∫–æ–Ω—Ç–µ—Å—Ç": set(["–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"]),
    }

    for token in doc.tokens:
        if token.lemma in programming_indicators:
            disciplines.extend(programming_indicators[token.lemma])

    return list(set(disciplines))


def extract_participant_count(text: str) -> Optional[int]:
    """Extract participant count using Natasha NLP."""
    doc = Doc(text)
    doc.segment(segmenter)
    doc.tag_morph(morph_tagger)

    text_lower = text.lower()

    # First check for team size patterns
    team_patterns = [
        r"–∫–æ–º–∞–Ω–¥[–∞-—è]*\s+(?:–ø–æ|–∏–∑|—á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å—é)\s+(\d+)(?:\s*-\s*(\d+))?\s+—á–µ–ª–æ–≤–µ–∫",
        r"—Å–æ—Å—Ç–∞–≤[–∞-—è]*\s+–∫–æ–º–∞–Ω–¥[–∞-—è]*\s*[:]\s*(\d+)(?:\s*-\s*(\d+))?\s+—á–µ–ª–æ–≤–µ–∫",
        r"–∫–æ–º–∞–Ω–¥[–∞-—è]*\s+(?:–ø–æ|–∏–∑|—á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å—é)\s+(\d+)(?:\s*-\s*(\d+))?\s+—É—á–∞—Å—Ç–Ω–∏–∫",
        r"—Å–æ—Å—Ç–∞–≤[–∞-—è]*\s+–∫–æ–º–∞–Ω–¥[–∞-—è]*\s*[:]\s*(\d+)(?:\s*-\s*(\d+))?\s+—É—á–∞—Å—Ç–Ω–∏–∫",
    ]

    for pattern in team_patterns:
        match = re.search(pattern, text_lower)
        if match:
            team_size = match.groups()
            if len(team_size) == 2 and team_size[1]:  # Range given
                avg_size = (int(team_size[0]) + int(team_size[1])) // 2
            else:
                avg_size = int(team_size[0])

            # Look for number of teams
            team_count_patterns = [
                r"(\d+)\s+–∫–æ–º–∞–Ω–¥",
                r"–∫–æ–º–∞–Ω–¥\s*[:]\s*(\d+)",
                r"–∑–∞—è–≤–∏–ª–æ—Å—å\s+(\d+)\s+–∫–æ–º–∞–Ω–¥",
            ]

            for team_pattern in team_count_patterns:
                team_count_match = re.search(team_pattern, text_lower)
                if team_count_match:
                    return int(team_count_match.group(1)) * avg_size

    # Common patterns for participant counts
    count_patterns = [
        # Exact counts
        r"(\d+)\s+(?:—É—á–∞—Å—Ç–Ω–∏–∫(?:–∞|–æ–≤)?|—á–µ–ª–æ–≤–µ–∫(?:–∞)?|—Å–ø–æ—Ä—Ç—Å–º–µ–Ω(?:–∞|–æ–≤)?)",
        r"—É—á–∞—Å—Ç–Ω–∏–∫–æ–≤?\s*[:]\s*(\d+)",
        r"–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–æ—Å—å\s+(\d+)",
        r"(?:–ø—Ä–∏–º—É—Ç|–ø—Ä–∏–Ω—è–ª–∏)\s+—É—á–∞—Å—Ç–∏–µ\s+(\d+)",
        r"(?:—É—á–∞—Å—Ç–≤—É–µ—Ç|—É—á–∞—Å—Ç–≤—É—é—Ç|—É—á–∞—Å—Ç–≤–æ–≤–∞–ª–æ)\s+(\d+)",
        r"(?:—Å–æ–±—Ä–∞–ª(?:–∞|–æ|–∏)?|—Å–æ–±—Ä–∞–ª–∏—Å—å)\s+(\d+)",
        # Registration counts
        r"–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ\s+(\d+)",
        r"—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–π\s*[:]\s*(\d+)",
        r"–∑–∞—è–≤–æ–∫\s*[:]\s*(\d+)",
        r"–∑–∞—è–≤–∏–ª–æ—Å—å\s+(\d+)",
        # Range patterns (take the upper bound)
        r"–æ—Ç\s+\d+\s+–¥–æ\s+(\d+)\s+(?:—É—á–∞—Å—Ç–Ω–∏–∫|—á–µ–ª–æ–≤–µ–∫|—Å–ø–æ—Ä—Ç—Å–º–µ–Ω)",
        r"(\d+)(?:\s*-\s*|\s+–¥–æ\s+)(\d+)\s+(?:—É—á–∞—Å—Ç–Ω–∏–∫|—á–µ–ª–æ–≤–µ–∫|—Å–ø–æ—Ä—Ç—Å–º–µ–Ω)",
    ]

    # Try exact patterns first
    for pattern in count_patterns:
        match = re.search(pattern, text_lower)
        if match:
            groups = match.groups()
            if len(groups) == 2:  # Range pattern
                return max(int(groups[0]), int(groups[1]))
            count = int(groups[0])
            # Validate the count is reasonable
            if 1 <= count <= 10000:  # Reasonable limits for a programming event
                return count

    # Check for approximate counts
    approx_patterns = [
        (r"–±–æ–ª–µ–µ\s+(\d+)\s+(?:—É—á–∞—Å—Ç–Ω–∏–∫|—á–µ–ª–æ–≤–µ–∫|—Å–ø–æ—Ä—Ç—Å–º–µ–Ω)", 1.0),  # exact number
        (r"—Å–≤—ã—à–µ\s+(\d+)\s+(?:—É—á–∞—Å—Ç–Ω–∏–∫|—á–µ–ª–æ–≤–µ–∫|—Å–ø–æ—Ä—Ç—Å–º–µ–Ω)", 1.0),  # exact number
        (r"–æ–∫–æ–ª–æ\s+(\d+)\s+(?:—É—á–∞—Å—Ç–Ω–∏–∫|—á–µ–ª–æ–≤–µ–∫|—Å–ø–æ—Ä—Ç—Å–º–µ–Ω)", 1.0),  # exact number
        (r"–ø–æ—Ä—è–¥–∫–∞\s+(\d+)\s+(?:—É—á–∞—Å—Ç–Ω–∏–∫|—á–µ–ª–æ–≤–µ–∫|—Å–ø–æ—Ä—Ç—Å–º–µ–Ω)", 1.0),  # exact number
        (r"–ø–æ—á—Ç–∏\s+(\d+)\s+(?:—É—á–∞—Å—Ç–Ω–∏–∫|—á–µ–ª–æ–≤–µ–∫|—Å–ø–æ—Ä—Ç—Å–º–µ–Ω)", 0.95),  # slightly less
        (r"–ø—Ä–∏–º–µ—Ä–Ω–æ\s+(\d+)\s+(?:—É—á–∞—Å—Ç–Ω–∏–∫|—á–µ–ª–æ–≤–µ–∫|—Å–ø–æ—Ä—Ç—Å–º–µ–Ω)", 1.0),  # exact number
        (r"–±–æ–ª–µ–µ\s+(\d+)", 1.0),  # exact number
        (r"—Å–≤—ã—à–µ\s+(\d+)", 1.0),  # exact number
        (r"–æ–∫–æ–ª–æ\s+(\d+)", 1.0),  # exact number
        (r"–ø–æ—Ä—è–¥ÔøΩÔøΩ–∞\s+(\d+)", 1.0),  # exact number
    ]

    for pattern, multiplier in approx_patterns:
        match = re.search(pattern, text_lower)
        if match:
            count = int(match.group(1))
            # Validate the count is reasonable
            if 1 <= count <= 10000:  # Reasonable limits for a programming event
                return int(count * multiplier)

    # Look for numbers near participant-related words
    participant_words = {
        "—É—á–∞—Å—Ç–Ω–∏–∫",
        "—Å–ø–æ—Ä—Ç—Å–º–µ–Ω",
        "—á–µ–ª–æ–≤–µ–∫",
        "–∫–æ–º–∞–Ω–¥–∞",
        "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è",
        "–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏—Å—å",
        "–∞—Ç–ª–µ—Ç",
        "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç",
        "—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫",
        "—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç",
    }

    # Create a list of tokens with their positions
    for token in doc.tokens:
        if token.lemma in participant_words:
            # Look for numbers in nearby context (within 5 tokens)
            context_start = max(0, token.start - 50)
            context_end = min(len(text_lower), token.stop + 50)
            context = text_lower[context_start:context_end]

            # Try to find numbers in the context
            numbers = re.findall(r"(\d+)", context)
            if numbers:
                # Convert all numbers to integers and take the largest one
                # that's reasonable (less than 10000)
                valid_numbers = [int(n) for n in numbers if 1 <= int(n) <= 10000]
                if valid_numbers:
                    return max(valid_numbers)

    return None


def process_message(message: dict) -> Optional[Event]:
    """Process a single message and extract event information if present."""
    # Skip service messages and messages without text
    if message.get("type") == "service" or "text" not in message:
        return None

    # Get the text content
    text = ""
    if isinstance(message["text"], str):
        text = message["text"]
    elif isinstance(message["text"], list):
        for item in message["text"]:
            if isinstance(item, str):
                text += item
            elif isinstance(item, dict) and "text" in item:
                text += item["text"]

    text_lower = text.lower()

    # Keywords that indicate an event announcement (future events)
    announcement_keywords = [
        "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ—Ç–∫—Ä—ã—Ç–∞",
        "–ø—Ä–∏–≥–ª–∞—à–∞–µ–º",
        "—Å–æ—Å—Ç–æ–∏—Ç—Å—è",
        "–ø—Ä–æ–π–¥–µ—Ç",
        "–ø—Ä–æ–≤–æ–¥–∏—Ç",
        "–æ–±—ä—è–≤–ª—è–µ—Ç",
        "–Ω–∞—á–∞–ª–æ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏",
        "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –¥–æ—Å—Ç—É–ø–Ω–∞",
        "–ø—Ä–∏–≥–ª–∞—à–∞—é—Ç—Å—è",
        "–æ—Ç–∫—Ä—ã—Ç –Ω–∞–±–æ—Ä",
        "–ø—Ä–æ–≤–µ–¥–µ—Ç",
        "—Å—Ç–∞—Ä—Ç—É–µ—Ç",
        "–æ—Ç–∫—Ä—ã–≤–∞–µ—Ç—Å—è",
        "–Ω–∞—á–Ω–µ—Ç—Å—è",
        "–ø—Ä–∏–≥–ª–∞—à–∞—é—Ç—Å—è",
        "–∂–¥–µ–º –≤–∞—Å",
        "—É—Å–ø–µ–π—Ç–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è",
        "—Å–ø–µ—à–∏—Ç–µ —É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å",
        "–Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–µ",
    ]

    # Keywords that indicate past events or news
    past_event_keywords = [
        "–ø—Ä–æ—à–µ–ª",
        "–ø—Ä–æ—à–ª–∞",
        "–ø—Ä–æ—à–ª–æ",
        "–ø—Ä–æ—à–ª–∏",
        "—Å–æ—Å—Ç–æ—è–ª—Å—è",
        "—Å–æ—Å—Ç–æ—è–ª–∞—Å—å",
        "—Å–æ—Å—Ç–æ—è–ªÔøΩÔøΩ—Å—å",
        "—Å–æ—Å—Ç–æ—è–ª–∏—Å—å",
        "–∑–∞–≤–µ—Ä—à–∏–ª—Å—è",
        "–∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å",
        "–∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å",
        "–∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å",
        "–ø–æ–¥–≤–µ–¥–µ–Ω—ã –∏—Ç–æ–≥–∏",
        "—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã",
        "–ø–æ–±–µ–¥–∏—Ç–µ–ª–∏",
        "–ø—Ä–∏–∑–µ—Ä—ã",
        "—Å—Ç–∞–ª–∏",
        "–≤—ã–∏–≥—Ä–∞–ª–∏",
        "–∑–∞–Ω—è–ª–∏",
        "–ø–æ–ª—É—á–∏–ª–∏",
        "–ø–æ–∑–¥—Ä–∞–≤–ª—è–µ–º",
    ]

    event_keywords = [
        "—Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ",
        "—Ç—É—Ä–Ω–∏—Ä",
        "—á–µ–º–ø–∏–æ–Ω–∞—Ç",
        "–æ–ª–∏–º–ø–∏–∞–¥–∞",
        "–∫–æ–Ω—Ç–µ—Å—Ç",
        "–∫–æ–Ω–∫—É—Ä—Å",
        "—Ö–∞–∫–∞—Ç–æ–Ω",
        "–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ",
        "–ø–µ—Ä–≤–µ–Ω—Å—Ç–≤–æ",
    ]

    # Check if this message is about a past event
    is_past_event = any(keyword in text_lower for keyword in past_event_keywords)
    if is_past_event:
        return None

    # Check if this message is an event announcement
    is_announcement = any(keyword in text_lower for keyword in announcement_keywords)
    has_event = any(keyword in text_lower for keyword in event_keywords)

    # Skip if not an event announcement
    if not (is_announcement and has_event):
        return None

    event = Event()

    # Extract title (first line or sentence that contains event keywords)
    sentences = text.split("\n")
    for sentence in sentences:
        sentence_lower = sentence.lower()
        if any(keyword in sentence_lower for keyword in event_keywords):
            # Clean up the title
            title = sentence.strip()
            # Remove common prefixes that might appear in announcements
            prefixes_to_remove = [
                "üéâ",
                "üì¢",
                "‚ùóÔ∏è",
                "‚ùó",
                "‚ÄºÔ∏è",
                "‚ö°Ô∏è",
                "üî•",
                "üì£",
                "üí•",
                "–í–Ω–∏–º–∞–Ω–∏–µ!",
                "–ê–Ω–æ–Ω—Å:",
                "–û–±—ä—è–≤–ª–µ–Ω–∏–µ:",
                "üèÜ",
                "üî∑",
                "üéä",
            ]
            for prefix in prefixes_to_remove:
                if title.startswith(prefix):
                    title = title[len(prefix) :].strip()
            event.title = title
            break

    # Extract description (rest of the text after title)
    if event.title and event.title in text:
        description_text = text[text.index(event.title) + len(event.title) :].strip()
        if description_text:
            event.description = description_text

    # Create Natasha Doc for text analysis
    doc = Doc(text)
    doc.segment(segmenter)
    doc.tag_morph(morph_tagger)

    # Extract disciplines first as they might help with age inference
    event.discipline = extract_disciplines(text)

    # Extract age range with context from disciplines
    age_min, age_max = extract_age_range(text)

    # If no explicit age range found, try to infer from context and discipline
    if age_min is None or age_max is None:
        # Check for specific age-related words
        if "–º–æ–ª–æ–¥–µ–∂" in text_lower:
            age_min = age_min or 14
            age_max = age_max or 35
        elif "—à–∫–æ–ª—å–Ω–∏–∫" in text_lower:
            age_min = age_min or 14
            age_max = age_max or 18
        elif "—Å—Ç—É–¥–µ–Ω—Ç" in text_lower:
            age_min = age_min or 17
            age_max = age_max or 25

        # Check for discipline-specific defaults
        if (
            "—Ö–∞–∫–∞—Ç–æ–Ω" in text_lower
            or "–ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ" in event.discipline
        ):
            if "—Ç–∞–ª–∞–Ω—Ç–ª–∏–≤" in text_lower and "—Å–ø–æ—Ä—Ç—Å–º–µ–Ω" in text_lower:
                age_min = age_min or 14
                age_max = age_max or None
            else:
                age_min = age_min or 16
                age_max = age_max or None
        elif "–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ" in event.discipline:
            if "–¥–æ—Å—Ç–∏–≥—à–∏—Ö" in text_lower:
                match = re.search(r"–¥–æ—Å—Ç–∏–≥—à–∏—Ö\s+(\d+)", text_lower)
                if match:
                    age_min = int(match.group(1))
            else:
                age_min = age_min or 14
                age_max = age_max or 25

        # Check for participant type hints
        if "—Ä–µ–±—è—Ç" in text_lower:
            age_min = age_min or 14
            age_max = age_max or 25

    # Validate and normalize age range
    if age_min is not None and age_max is not None:
        if age_min > age_max:
            age_min, age_max = age_max, age_min

    # Special case for hackathons with prize money
    if "—Ö–∞–∫–∞—Ç–æ–Ω" in text_lower and re.search(
        r"\d+\s*(?:000|–º–ª–Ω|—Ç—ã—Å[–∞-—è]*)\s*—Ä—É–±", text_lower
    ):
        age_min = 16  # Most hackathons with prize money require participants to be at least 16
        age_max = None

    event.age_min = age_min
    event.age_max = age_max

    # Extract dates
    event.start_date = extract_date(text)
    event.end_date = extract_date(text)

    # Extract location
    event.location = extract_location(text)

    # Extract participant count
    event.participant_count = extract_participant_count(text)

    # Only return events that have at least a title and either a start date or location
    if event.title and (event.start_date or event.location):
        return event
    return None


def process_chat_history(file_path: str) -> List[Dict]:
    """Process a Telegram chat history file and extract events."""
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    events = []
    for message in data.get("messages", []):
        event = process_message(message)
        if event and event.title:  # Only include events with at least a title
            events.append(event.to_dict())

    return events


def main():
    # Process both chat histories
    parser = argparse.ArgumentParser(
        description="Extract events from Telegram chat history."
    )
    parser.add_argument("path", type=str, help="Path to the JSON chat history file")
    args = parser.parse_args()
    path_to_json = args.path

    # Process Kaliningrad events
    all_events = process_chat_history(path_to_json)
    print(f"\nTotal events detected: {len(all_events)}")

    # Write all events to a single file
    with open("extracted_events.json", "w", encoding="utf-8") as f:
        json.dump(
            {"events": all_events},
            f,
            ensure_ascii=False,
            indent=2,
            default=lambda x: x.__dict__,
        )


if __name__ == "__main__":
    main()
